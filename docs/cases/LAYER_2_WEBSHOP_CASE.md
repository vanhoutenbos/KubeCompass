# Layer 2: Platform Enhancements & Resilience â€” Decision Framework

**Target Audience**: Platform Engineers, DevOps Engineers, SREs, Architects  
**Type**: Decision Framework & Capability Mapping  
**Organization**: Dutch webshop / online store with Essential SAFe methodology  
**Prerequisite**: Layer 0 and Layer 1 are **analyzed and structured** (not necessarily implemented)  

---

## âš ï¸ Important: This is NOT an Implementation Guide

**This document is:**
- ğŸ¯ A **decision framework** for when Layer 2 capabilities become relevant
- ğŸ§­ A **capability map** with choice space and trade-offs
- ğŸ’¡ A **thinking framework** for mature platform teams
- ğŸ“š A **reference** for architecture decisions
- ğŸ¤– **Input for AI agents** to reason about architecture choices

**This document is NOT:**
- âŒ A deployment guide
- âŒ A Helm/Terraform tutorial
- âŒ A "copy-paste and deploy" manual
- âŒ A replacement for vendor documentation
- âŒ An implementation you can run 1-to-1

---

## Reading Guide

ğŸ¯ **[TRIGGER]** - When does this capability become relevant?  
ğŸ”€ **[TRADE-OFFS]** - What choices do you have and what are the trade-offs?  
âš ï¸ **[TIMING]** - Why implement now or not?  
ğŸ’­ **[DECISION POINT]** - What questions must you be able to answer?  
ğŸ”— **[LAYER 1 LINK]** - How does this build on Layer 1?  

---

## Executive Summary

This document describes **Layer 2: Platform Enhancements & Resilience** â€” the **decision-making process** for advanced platform capabilities after the Layer 1 foundation is stable.

### Context: From Layer 1 to Layer 2

**Layer 1 (Foundation)** is about:
- Cluster runs
- GitOps works
- Basic observability (metrics, logs)
- Network policies exist
- Deployments succeed

**Layer 2 (Enhancement)** is about **maturity and optimization**:
- What happens **between** services? â†’ service mesh
- How do you **trace** a request end-to-end? â†’ distributed tracing
- How do you **test** failure scenarios? â†’ chaos engineering
- How do you **enforce** security policies? â†’ policy enforcement
- Where does the **money** go? â†’ cost visibility
- Are we **ready** for multi-region? â†’ architectural readiness

### Core Question Layer 2

> **"When does this complexity investment become worthwhile?"**

Not: "What tool is best?"  
But: "**Under what circumstances would I even want this?**"

---

## Decision Matrix: Layer 1 â†’ Layer 2 Triggers

| Capability | Trigger Condition | Complexity Cost | Skip If... |
|-----------|-------------------|----------------|------------|
| **Service Mesh** | > 5 microservices, security/observability per-service | Medium-High | Monolith or < 3 services |
| **Distributed Tracing** | Debugging cross-service issues takes > 1h | Medium | < 5 services, simple call chains |
| **Chaos Engineering** | Production incidents, HA validation needed | Low-Medium | Dev environment, single instance |
| **Policy Enforcement** | Compliance requirement, > 10 developers | Medium | Small team, manual review works |
| **Cost Visibility** | Budget concerns, multi-tenant | Low | Single team, fixed costs |
| **Multi-Region Readiness** | Latency requirements, DR strategy | High | Single region sufficient, no DR requirement |
| **Enhanced Auditing** | GDPR/DORA compliance, security team | Medium | No compliance requirement |

---

## 1. Service Mesh

### ğŸ¯ [TRIGGER] When Relevant?

**Implement a service mesh when:**
- âœ… You have **> 5 microservices** with complex inter-service communication
- âœ… You want **per-service security** (mTLS without code changes)
- âœ… You need **detailed service-level metrics** (latency, error rate, traffic)
- âœ… You want to do **canary deployments** or **traffic splitting**
- âœ… You want to visualize **service topology** and **dependency mapping**

**NOT implement when:**
- âŒ You have a **monolith** or **< 3 services**
- âŒ Your team has **no capacity** for additional operational overhead
- âŒ **Network policies (L3/L4)** are sufficient for your security model
- âŒ Basic Prometheus metrics provide enough information

### ğŸ”€ [TRADE-OFFS] Choice Space

| Tool | Pro | Contra | Use When |
|------|-----|--------|----------|
| **Linkerd** | Smallest footprint, simple, auto-mTLS | Fewer features than Istio | Small team, simple use case |
| **Istio** | Feature-rich, enterprise support, mature | Complex, resource-intensive | Large org, complex routing |
| **Cilium Service Mesh** | eBPF-based (very performant), CNI-integration | Beta, less mature | Already Cilium CNI, early adopter |
| **Consul Connect** | Multi-datacenter native, HashiCorp stack | Requires Consul infra | Already HashiCorp stack (Vault, Nomad) |

### ğŸ’­ [DECISION POINT] Questions to Answer

1. **Do we have a problem now that a service mesh solves?**
   - Have we had incidents through miscommunication between services?
   - Are we missing visibility in service-to-service latencies?
   - Is our current security model (network policies) insufficient?

2. **Can we handle the operational overhead?**
   - Do we have capacity for learning curve?
   - Can we debug sidecar injection?
   - Do we have monitoring for mesh health?

3. **What is our exit strategy?**
   - Can we return to no mesh without major refactor?
   - Are we vendor-locked?

### âš ï¸ [TIMING] Why Now or Not?

**Implement NOW if:**
- You run microservices in production
- You require security compliance (mTLS everywhere)
- Debugging cross-service issues takes > 4 hours per week

**Wait LATER if:**
- Layer 1 is not yet stable
- You have < 5 services
- Team has no Kubernetes experience yet

### ğŸ”— [LAYER 1 LINK]

**Builds on:**
- Cilium CNI (Layer 1) â†’ Service mesh adds L7 visibility
- Prometheus (Layer 1) â†’ Service mesh adds per-service golden signals
- Network Policies (Layer 1) â†’ Service mesh adds mTLS (zero-trust)

**Does not replace:**
- Network policies remain relevant (defense in depth)
- Cilium CNI remains, service mesh is additive

---

## 2. Distributed Tracing

### ğŸ¯ [TRIGGER] When Relevant?

**Implement distributed tracing when:**
- âœ… You have **> 5 microservices** with **complex call chains**
- âœ… **Debugging cross-service issues** takes > 1 hour per incident
- âœ… You need **root cause analysis** of performance problems
- âœ… You want to understand **service dependencies** (dependency graph)
- âœ… You want **correlation** between logs/metrics/traces (observability maturity)

**NOT implement when:**
- âŒ You have **< 5 services** with simple call chains
- âŒ **Logs + metrics** are sufficient for debugging
- âŒ You have no **storage budget** for traces (high data volume)
- âŒ Team has no time for **service instrumentation**

### ğŸ”€ [TRADE-OFFS] Choice Space

| Approach | Pro | Contra | Use When |
|----------|-----|--------|----------|
| **OpenTelemetry + Jaeger** | Open standard, vendor-neutral, mature | Storage overhead, setup complexity | Self-hosted, vendor independence |
| **OpenTelemetry + Tempo** | Grafana native, S3 storage, cost-efficient | Newer than Jaeger, less features | Already Grafana stack, S3 available |
| **Cloud Provider (X-Ray, AppInsights)** | Managed, auto-instrumentation | Vendor lock-in, costs | Already in that cloud, no self-host |
| **Zipkin** | Lightweight, simple | Minder actief dan Jaeger | Legacy use case |

### ğŸ’­ [DECISION POINT] Questions to Answer

1. **What is the business impact of slow debugging?**
   - How many tijd takes cross-service debugging per week?
   - How many klantimpact hebband performance issues?

2. **Can we services instrumenterand?**
   - Do we have ownership per service (for SDK integration)?
   - Can we auto-instrumentation gebruikand (.NET, Java)?
   - Do we have capacity for manual instrumentation (Go, Python)?

3. **What is our trace retention strategy?**
   - How many days traces store? (storage costs!)
   - Sampling ratio (100% dev, 10% prod, 1% for high-traffic)?

### âš ï¸ [TIMING] Why Now or Not?

**Implement NOW if:**
- Debugging cross-service issues > 1 uur per incident
- You production incidents have without clear root cause
- You service depanddencies onbekend are (shadow depanddencies)

**Wait LATER if:**
- You < 5 services have
- Logs + metrics sufficient are
- No budget for trace storage (TB's per month!)

### ğŸ”— [LAYER 1 LINK]

**Builds on:**
- Prometheus (Layer 1) â†’ Traces voegand request-level detail add
- Loki (Layer 1) â†’ Trace ID in logs for correlatie
- Grafana (Layer 1) â†’ Unified view (metrics + logs + traces)

---

## 3. Chaos Enginoring

### ğŸ¯ [TRIGGER] When Relevant?

**Implementeer chaos andginoring when:**
- âœ… You **HA (High Availability) claimt** maar niet test
- âœ… You **production incidents** want voorkomand through proactief testand
- âœ… You **RTO/RPO** want validerand (Disaster Recovery testing)
- âœ… You **team confidence** want bouwand in platform resilience
- âœ… You **SLO's** have die je want validerand

**NOT implement when:**
- âŒ You **no HA setup** have (single replica, single node)
- âŒ You **Layer 1 basis not stable** is
- âŒ Team **no tijd** has for experiment analysis
- âŒ You **no monitoring** have to impact to measure

### ğŸ”€ [TRADE-OFFS] Choice Space

| Tool | Pro | Contra | Use When |
|------|-----|--------|----------|
| **Chaos Mesh** | K8s-native, rich scenarios, GitOps | Chinos project (governance concern?) | Self-hosted, veel scenarios |
| **LitmusChaos** | CNCF project, community-drivand | Complex setup | CNCF preference |
| **Gremlin** | Enterprise support, managed | Commercial, niet self-hosted | Budget available, managed voorkeur |
| **AWS/Azure Chaos** | Cloud-native, provider integration | Vendor lock-in | Already in that cloud |

### ğŸ’­ [DECISION POINT] Questions to Answer

1. **What is our chaos maturity?**
   - Do we have HA setup (meerdere replicas, nodes)?
   - Do we have PodDisruptionBudgets?
   - Do we have readiness/liveness probes?

2. **What failure scenarios are relevant?**
   - Pod crash? (test: K8s restart works)
   - Node failure? (test: pod scheduling on andere node)
   - Network partition? (test: services blijvand available)
   - Resource stress? (test: HPA scaling works)

3. **How do we measure success?**
   - SLO's blijvand within target? (99.9% uptime maintained)
   - Alerts triggerand correct?
   - Automated recovery works?

### âš ï¸ [TIMING] Why Now or Not?

**Implement NOW if:**
- You HA claimt maar nooit test
- You production incidents have through failure scenarios
- You confidence want bouwand in platform resilience

**Wait LATER if:**
- Layer 1 nog not stable is
- You no HA setup have
- No monitoring to impact to measure

### ğŸ”— [LAYER 1 LINK]

**Builds on:**
- Velero (Layer 1) â†’ Chaos test: cluster restore works?
- Prometheus (Layer 1) â†’ Chaos experiments zichtbaar in metrics
- HA setup (Layer 1) â†’ Chaos valideert dat HA daadwerkelijk works

---

## 4. Policy Enforcement (Low Trust)

### ğŸ¯ [TRIGGER] When Relevant?

**Implementeer policy enforcement when:**
- âœ… You **> 10 developers** have die niet allemaal K8s-experts are
- âœ… You **compliance verrequirementtand** have (GDPR, DORA, PCI-DSS)
- âœ… You **security incidents** have gehad through misconfigurations
- âœ… You **automated validation** want for deployments
- âœ… You **audit trail** needed have for wie-wat-whand

**NOT implement when:**
- âŒ You **< 5 developers** have die allemaal K8s-experts are
- âŒ **Manowal review** proces works goed
- âŒ No compliance requirements
- âŒ Team kan policy complexity niet aan

### ğŸ”€ [TRADE-OFFS] Choice Space

| Tool | Pro | Contra | Use When |
|------|-----|--------|----------|
| **Kyverno** | YAML-based (no Rego), mutations, generate policies | Minder krachtig dan OPA | Developer-friendly, eenvoud |
| **OPA Gatekeeper** | Zeer krachtig (Rego), mature, CNCF | Rego learning curve, complex | Security team, complexe policies |
| **Kubewardand** | Policies in Rust/Go/etc, WebAssembly | Nieuw, kleine community | Bleeding edge, WASM fans |
| **Pod Security Admission** | K8s native, gratis | Alleand pod security, niet extensible | Basic security, no custom policies |

### ğŸ’­ [DECISION POINT] Questions to Answer

1. **What policies are kritisch?**
   - No privileged containers? (security critical)
   - Resource limits required? (capacity management)
   - Trusted registry only? (supply chain security)
   - Network policies required? (network security)

2. **What is our enforcement strategy?**
   - Start in **audit mode** (1 month: collect violations)
   - Gradual **warnings** (1 month: teams fixand violations)
   - Per-policy **andforce mode** (dev â†’ staging â†’ production)

3. **What is our exception strategy?**
   - Who kan policy exceptions goedkeurand?
   - How lang are exceptions geldig?
   - Audit trail for exceptions?

### âš ï¸ [TIMING] Why Now or Not?

**Implement NOW if:**
- You compliance requirements have
- You security incidents have through misconfigs
- You > 10 developers have

**Wait LATER if:**
- You < 5 developers have
- Manowal review works prima
- Team has no capacity for policy management

### ğŸ”— [LAYER 1 LINK]

**Builds on:**
- Pod Security Stenards (Layer 1) â†’ Policies andforce PSS automatisch
- Network Policies (Layer 1) â†’ Policy generates default-deny
- RBAC (Layer 1) â†’ Policy audit RBAC changes

---

## 5. Cost Visibility

### ğŸ¯ [TRIGGER] When Relevant?

**Implementeer cost visibility when:**
- âœ… You **budget concerns** have (costs rising unexpectedly)
- âœ… You **multi-tenant cluster** have (costs allocatie per team)
- âœ… You **showback/chargeback** want doand per team/project
- âœ… You **idle resources** want identificerand (cost optimization)
- âœ… You **capacity planning** want baserand on daadwerkelijk gebruik

**NOT implement when:**
- âŒ You **single tenant** are with **fixed costs**
- âŒ **Cloud costs are no concern** (budget amply sufficient)
- âŒ Basic CPU/memory metrics from Prometheus sufficient are
- âŒ Team has no tijd for cost optimization

### ğŸ”€ [TRADE-OFFS] Choice Space

| Tool | Pro | Contra | Use When |
|------|-----|--------|----------|
| **Kubecost** | Feature-rich, recommendations, multi-cloud | Commercial (gratis versie OK) | Full-featured, single cluster |
| **OpenCost** | 100% opand-source, CNCF senbox | Minder features dan Kubecost | Budget constraint, basics |
| **Cloud Provider Tools** | Native integration, managed | Vendor lock-in, not K8s-native | Already in that cloud, native voorkeur |

### ğŸ’­ [DECISION POINT] Questions to Answer

1. **What willand we wetand?**
   - Kostand per namespace? (multi-tenancy)
   - Kostand per service? (microservices cost attribution)
   - Idle resource cost? (optimization opportunities)
   - Trend analysis? (groei voorspelland)

2. **What doand we with de data?**
   - Showback (informative)? â†’ OpenCost sufficient
   - Chargeback (financieel)? â†’ Kubecost aanbevoland
   - Optimization (rightsizing)? â†’ Kubecost recommendations

3. **What is our cost optimization strategy?**
   - Automated rightsizing? (risky!)
   - Manowal review (monthly)? (safe)
   - Alert on anomalies? (proactive)

### âš ï¸ [TIMING] Why Now or Not?

**Implement NOW if:**
- Kostand rising unexpectedly (> 20% per month)
- You multi-tenant bent (costs allocatie unclear)
- Management vraagt cost visibility

**Wait LATER if:**
- Single tenant, fixed costs
- Budget amply sufficient
- No capacity for cost optimization

### ğŸ”— [LAYER 1 LINK]

**Builds on:**
- Prometheus (Layer 1) â†’ Kubecost gebruikt CPU/memory metrics
- Resource limits (Layer 1) â†’ Cost tool toont waste (limits vs usage)

---

## 6. Multi-Region Readiness

### ğŸ¯ [TRIGGER] When Relevant?

**Overweeg multi-region when:**
- âœ… You **gebruikers in meerdere regio's** have with **latency requirements**
- âœ… You **Disaster Recovery** strategy multi-region verrequirementt (RTO < 1h)
- âœ… You **data residency** requirements have (GDPR: EU data in EU)
- âœ… You **traffic growth** verwacht die single region niet aankan

**NOT implement when:**
- âŒ **Single region sufficient** for je use case
- âŒ You **no DR requirement** have
- âŒ **Complexity** outweighs benefits (multi-region is HARD)
- âŒ You **database** is niet multi-region ready (biggest blocker!)

### ğŸ”€ [TRADE-OFFS] Architectural Choices

| Approach | Pro | Contra | Use When |
|----------|-----|--------|----------|
| **Active-Active (beide regio's live)** | Beste latency, beste HA | Zeer complex, data sync issues | Mission-critical, budget |
| **Active-Passive (1 live, 1 stenby)** | Simpeler, DR capability | Stenby idle (costs), RTO > 5 min | DR requirement, niet latency |
| **Read Replicas (DB per regio)** | Beste latency, simpeler | Eventual consistency, complex writes | Read-heavy workload |

### ğŸ’­ [DECISION POINT] Questions to Answer

1. **What is our multi-region strategy?**
   - **NOT NOW**: Single region in Layer 2
   - **LATER**: Multi-region in Layer 3 (indiand needed)
   - **PREPAREDNESS**: Architectuur must het niet blokkerand

2. **Is our architectuur multi-region ready?**
   - âœ… **Stateless services** (horizontally scalable)
   - âœ… **Shared database** (read replicas possible)
   - âœ… **Session state in cache** (Redis/Valkey kan regionaal)
   - â“ **Database replicatie** (biggest challenge!)

3. **What are our blockers?**
   - Database (PostgreSQL multi-region replicatie?)
   - Stateful workloads (persistent volumes regional)
   - Cost (double infrastructure)

### âš ï¸ [TIMING] Why Now or Not?

**Voorbereidand NU (niet activerand!):**
- Architectuur must multi-region niet blokkerand
- Cilium Cluster Mesh optie andabled (niet gebruikt)
- DR scenario's documenterand

**Activerand LATER (Layer 3):**
- Alleand as business case duidelijk is
- Database strategy duidelijk is
- Team capacity has

### ğŸ”— [LAYER 1 LINK]

**Builds on:**
- Cilium CNI (Layer 1) â†’ Cluster Mesh capability for multi-region
- Velero (Layer 1) â†’ Backup restore in andere regio (DR)
- Stateless design (Layer 1) â†’ Maakt multi-region possible

---

## 7. Observability Uitbreiding

### ğŸ¯ [TRIGGER] When Relevant?

**Verhoog observability maturity when:**
- âœ… You **Layer 1 observability** (metrics, logs) niet sufficient is
- âœ… You **alerting strategy** unclear is (alles is pager-worthy?)
- âœ… You **correlation** between metrics/logs/traces want
- âœ… You **SLO's** want monitorand (error budget)

### ğŸ”€ [TRADE-OFFS] Alerting Strategie

| Alert Level | Trigger | Action | Destination | Example |
|------------|---------|--------|-------------|---------|
| **CRITICAL** | Service down, data loss risk | Page on-call | PagerDuty | App down, DB unreachable |
| **WARNING** | Degraded performance | Notify team | Slack | High latency, high CPU |
| **INFO** | Routine events | Log only | Dashboard | Deployment success, backup OK |

### ğŸ’­ [DECISION POINT] Questions to Answer

1. **What is pager-worthy?**
   - **CRITICAL**: Production down, payment failures, data loss
   - **WARNING**: Slow performance, high resource usage
   - **INFO**: Successful deployments, routine events

2. **How correlerand we observability data?**
   - Trace ID in logs (structured logging)
   - Exemplars in Prometheus (metrics â†’ traces)
   - Grafana Explore (unified view)

3. **What is our SLO strategy?**
   - Define SLO's (99.9% uptime, P95 < 200ms)
   - Monitor error budget
   - Alert on budget burn rate

### âš ï¸ [TIMING] Why Now or Not?

**Verhoog maturity NU als:**
- Alert fatigue (te veel alerts, team negeert)
- Debugging is slow (no correlatie between metrics/logs/traces)
- SLO's are undefined (no error budget)

**Wait LATER if:**
- Layer 1 observability works prima
- Alert strategy is clear
- Team has no capacity

### ğŸ”— [LAYER 1 LINK]

**Builds on:**
- Prometheus (Layer 1) â†’ Enhanced with SLO monitoring
- Loki (Layer 1) â†’ Enhanced with structured logging + trace ID
- Grafana (Layer 1) â†’ Enhanced with unified observability

---

## 8. Security & Auditing

### ğŸ¯ [TRIGGER] When Relevant?

**Verhoog audit maturity when:**
- âœ… You **compliance requirements** have (GDPR, DORA, ISO 27001)
- âœ… You **security incidents** want kunnand investigerand
- âœ… You **break-glass procedures** needed have (emergency access)
- âœ… You **SIEM integration** want (centralized security monitoring)

**NOT implement when:**
- âŒ No compliance requirements
- âŒ Basic K8s audit logs sufficient are
- âŒ No security team

### ğŸ”€ [TRADE-OFFS] Audit Strategie

| Component | Audit What | Retention | Use When |
|-----------|------------|-----------|----------|
| **K8s Audit Logs** | API calls (kubectl, RBAC) | 1 year | Compliance |
| **ArgoCD Audit** | GitOps syncs, manual approvals | 1 year | Deployment audit |
| **Vault Audit** | Secret access (read/write) | 1 year | Secret compliance |

### ğŸ’­ [DECISION POINT] Questions to Answer

1. **What mustand we auditand?**
   - K8s API calls (wie deed wat whand)
   - GitOps syncs (welke change, through wie)
   - Secret access (wie las welke secret)
   - Break-glass usage (emergency access audit)

2. **What is our break-glass strategy?**
   - How krijg je emergency access? (privilege escalation tool)
   - Howlang geldig? (max 1 uur)
   - How loggand we dit? (full audit trail)
   - Post-incident review? (required)

3. **How integrerand we with SIEM?**
   - What SIEM? (Splunk, Elastic, etc.)
   - Push or pull? (Loki â†’ SIEM)
   - Real-time or batch? (batch = cheaper)

### âš ï¸ [TIMING] Why Now or Not?

**Implement NOW if:**
- Compliance requirements (DORA, GDPR)
- Security team vraagt audit trail
- Break-glass procedures needed

**Wait LATER if:**
- No compliance requirement
- Basic K8s audit logs sufficient
- No security team

### ğŸ”— [LAYER 1 LINK]

**Builds on:**
- RBAC (Layer 1) â†’ Audit RBAC changes
- Loki (Layer 1) â†’ Centralized audit log storage
- Vault (Layer 1) â†’ Audit secret access

---

## Resultaat Layer 2: Maturity Assessment

### Na Layer 2 Analyse Kun You Beantwoorden:

âœ… **Service Mesh**: Do we have dit needed? (> 5 services, security/observability)  
âœ… **Distributed Tracing**: Lost dit eand daadwerkelijk probleem op? (debugging > 1h)  
âœ… **Chaos Enginoring**: Testand we HA or claimand we het alleand?  
âœ… **Policy Enforcement**: Automated validation or manual review?  
âœ… **Cost Visibility**: Where does our money go?  
âœ… **Multi-Region**: Nu needed or later?  
âœ… **Observability**: What is pager-worthy?  
âœ… **Auditing**: Compliance requirement or overkill?  

### Layer 2 â†’ Layer 3 (Toekomst)

**Layer 3 zou gaan over:**
- Zero trust networking (full mutual TLS enforcement)
- Active multi-region (Cilium Cluster Mesh actief)
- Advanced chaos (automated, continowous)
- SLO-based automation (error budget policies)
- Cost optimization automation (rightsizing)
- Security automation (auto-remediation)

**Maar dat is alleand relevant as Layer 2 capabilities lopand en je **daadwerkelijk** complexity needed have.**

---

## Conclusion: Complexity as Bewuste Keuze

Layer 2 is niet "de volgende stap after Layer 1".  
Layer 2 is "**welke extra capabilities are de complexity waard?**"

### Beslisregels:

1. **Start simpel**: Layer 1 first, Layer 2 alleand as trigger duidelijk is
2. **EÃ©n tegelijk**: Niet alle Layer 2 capabilities tegelijk (team overload!)
3. **Measure impact**: Every capability must solve measurable problem
4. **Exit strategy**: Can we terug as het niet works?

### For the Webshop Case:

**Probably YES:**
- Distributed tracing (> 5 services, debugging issues)
- Cost visibility (budget concerns, multi-tenant)
- Policy enforcement (compliance, groeiend team)

**Probably NOT (now):**
- Service mesh (< 10 services, network policies sufficient)
- Multi-region (single region sufficient, no DR requirement)
- Chaos andginoring (first HA setup stabilize)

**Dit is de kracht or Layer 2: conscious, justified choices in plaats or "we doand alles maar".**

---

**Author**: [@vanhoutenbos](https://github.com/vanhoutenbos)  
**Version**: 2.0 (Decision Framework)  
**Date**: December 2024  
**License**: MIT
